{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "StatisticalFilter.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tfocbkXDz1o",
        "colab_type": "text"
      },
      "source": [
        "# Code for Statistical Filtering method\n",
        "\n",
        "In this file, the codes for our proposed statistical data filtering method are organized by the same order like the one in our paper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LehdaBQ-EaiO",
        "colab_type": "text"
      },
      "source": [
        "## Import needed libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9iTo7cHo5TAD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from numpy.random import seed\n",
        "from numpy.random import randn\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from collections import Counter\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import shutil\n",
        "import tensorflow.contrib.metrics as metrics\n",
        "from keras.layers import Flatten, Dense, BatchNormalization, Activation, Input, Dropout, LSTM\n",
        "from keras.regularizers import l2\n",
        "from keras.layers.merge import Concatenate\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.activations import selu\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.models import Sequential, Model\n",
        "from keras.optimizers import SGD, RMSprop\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chUsemRTGWiW",
        "colab_type": "text"
      },
      "source": [
        "## Data Wrangling\n",
        "- This step uses the data Excel file, modifies the headers a little and changes the type of file into .csv --> Please refer to the file DataMarked.csv in this folder\n",
        "- This section makes the data into 133 features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBWbHURNIy7D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load data into a Data frame\n",
        "df = pd.read_csv('DayMarked.csv')\n",
        "\n",
        "# Create an empty array with 133 features with array dimensions follow this structure: (day,hour,features)\n",
        "data = np.zeros((2975,24,133))\n",
        "\n",
        "#Set holiday variable at the feature #133 (Boolean: 0 - Not holiday; 1 - Holiday) \n",
        "data[:,0,-1] = df.values[:,-2]\n",
        "data[:,1,-1] = df.values[:,-2]\n",
        "data[:,2,-1] = df.values[:,-2]\n",
        "data[:,3,-1] = df.values[:,-2]\n",
        "data[:,4,-1] = df.values[:,-2]\n",
        "data[:,5,-1] = df.values[:,-2]\n",
        "data[:,6,-1] = df.values[:,-2]\n",
        "data[:,7,-1] = df.values[:,-2]\n",
        "data[:,8,-1] = df.values[:,-2]\n",
        "data[:,9,-1] = df.values[:,-2]\n",
        "data[:,10,-1] = df.values[:,-2]\n",
        "data[:,11,-1] = df.values[:,-2]\n",
        "data[:,12,-1] = df.values[:,-2]\n",
        "data[:,13,-1] = df.values[:,-2]\n",
        "data[:,14,-1] = df.values[:,-2]\n",
        "data[:,15,-1] = df.values[:,-2]\n",
        "data[:,16,-1] = df.values[:,-2]\n",
        "data[:,17,-1] = df.values[:,-2]\n",
        "data[:,18,-1] = df.values[:,-2]\n",
        "data[:,19,-1] = df.values[:,-2]\n",
        "data[:,20,-1] = df.values[:,-2]\n",
        "data[:,21,-1] = df.values[:,-2]\n",
        "data[:,22,-1] = df.values[:,-2]\n",
        "data[:,23,-1] = df.values[:,-2]\n",
        "\n",
        "#Set hour variables at features #2 to #25 (Boolean: 0 - Not in the desired hour, 1 - In the desire hour)\n",
        "data[:,0,1] = 1\n",
        "data[:,1,2] = 1\n",
        "data[:,2,3] = 1\n",
        "data[:,3,4] = 1\n",
        "data[:,4,5] = 1\n",
        "data[:,5,6] = 1\n",
        "data[:,6,7] = 1\n",
        "data[:,7,8] = 1\n",
        "data[:,8,9] = 1\n",
        "data[:,9,10] = 1\n",
        "data[:,10,11] = 1\n",
        "data[:,11,12] = 1\n",
        "data[:,12,13] = 1\n",
        "data[:,13,14] = 1\n",
        "data[:,14,15] = 1\n",
        "data[:,15,16] = 1\n",
        "data[:,16,17] = 1\n",
        "data[:,17,18] = 1\n",
        "data[:,18,19] = 1\n",
        "data[:,19,20] = 1\n",
        "data[:,20,21] = 1\n",
        "data[:,21,22] = 1\n",
        "data[:,22,23] = 1\n",
        "data[:,23,24] = 1\n",
        "\n",
        "#Set hourly values for the corresponding hour at feature #1 (Absolute values of hourly load data)\n",
        "data[:,0,0] = df.values[:,2]\n",
        "data[:,1,0] = df.values[:,3]\n",
        "data[:,2,0] = df.values[:,4]\n",
        "data[:,3,0] = df.values[:,5]\n",
        "data[:,4,0] = df.values[:,6]\n",
        "data[:,5,0] = df.values[:,7]\n",
        "data[:,6,0] = df.values[:,8]\n",
        "data[:,7,0] = df.values[:,9]\n",
        "data[:,8,0] = df.values[:,10]\n",
        "data[:,9,0] = df.values[:,11]\n",
        "data[:,10,0] = df.values[:,12]\n",
        "data[:,11,0] = df.values[:,13]\n",
        "data[:,12,0] = df.values[:,14]\n",
        "data[:,13,0] = df.values[:,15]\n",
        "data[:,14,0] = df.values[:,16]\n",
        "data[:,15,0] = df.values[:,17]\n",
        "data[:,16,0] = df.values[:,18]\n",
        "data[:,17,0] = df.values[:,19]\n",
        "data[:,18,0] = df.values[:,20]\n",
        "data[:,19,0] = df.values[:,21]\n",
        "data[:,20,0] = df.values[:,22]\n",
        "for i in range(len(df)):\n",
        "    data[i,21,0] = float(df.values[i,23])\n",
        "    data[i,22,0] = float(df.values[i,24])\n",
        "data[:,23,0] = df.values[:,25]\n",
        "\n",
        "#Set weekday & day-in-a-month & week-in-a-year & month-in-a-year & quarter-in-a-year \n",
        "#indices at features #26 to #132 (Boolean: 0 - Not in the desired indicator, 1 - In the desire indicator)\n",
        "\n",
        "for i in range(len(df)):\n",
        "    #Weekday indices - Boolean - from feature #26 to #32\n",
        "    if df.values[i,0] == 'CN':\n",
        "        data[i,0,31] = 1\n",
        "        data[i,1,31] = 1\n",
        "        data[i,2,31] = 1\n",
        "        data[i,3,31] = 1\n",
        "        data[i,4,31] = 1\n",
        "        data[i,5,31] = 1\n",
        "        data[i,6,31] = 1\n",
        "        data[i,7,31] = 1\n",
        "        data[i,8,31] = 1\n",
        "        data[i,9,31] = 1\n",
        "        data[i,10,31] = 1\n",
        "        data[i,11,31] = 1\n",
        "        data[i,12,31] = 1\n",
        "        data[i,13,31] = 1\n",
        "        data[i,14,31] = 1\n",
        "        data[i,15,31] = 1\n",
        "        data[i,16,31] = 1\n",
        "        data[i,17,31] = 1\n",
        "        data[i,18,31] = 1\n",
        "        data[i,19,31] = 1\n",
        "        data[i,20,31] = 1\n",
        "        data[i,21,31] = 1\n",
        "        data[i,22,31] = 1\n",
        "        data[i,23,31] = 1\n",
        "    else:\n",
        "        a = int(df.values[i,0])+23\n",
        "        data[i,0,a] = 1\n",
        "        data[i,1,a] = 1\n",
        "        data[i,2,a] = 1\n",
        "        data[i,3,a] = 1\n",
        "        data[i,4,a] = 1\n",
        "        data[i,5,a] = 1\n",
        "        data[i,6,a] = 1\n",
        "        data[i,7,a] = 1\n",
        "        data[i,8,a] = 1\n",
        "        data[i,9,a] = 1\n",
        "        data[i,10,a] = 1\n",
        "        data[i,11,a] = 1\n",
        "        data[i,12,a] = 1\n",
        "        data[i,13,a] = 1\n",
        "        data[i,14,a] = 1\n",
        "        data[i,15,a] = 1\n",
        "        data[i,16,a] = 1\n",
        "        data[i,17,a] = 1\n",
        "        data[i,18,a] = 1\n",
        "        data[i,19,a] = 1\n",
        "        data[i,20,a] = 1\n",
        "        data[i,21,a] = 1\n",
        "        data[i,22,a] = 1\n",
        "        data[i,23,a] = 1\n",
        "    #Day-in-a-month indices - Boolean - from feature #33 to #63 - 31 days\n",
        "    b = int(df.values[i,1][-5:-3])+31\n",
        "    data[i,0,b] = 1\n",
        "    data[i,1,b] = 1\n",
        "    data[i,2,b] = 1\n",
        "    data[i,3,b] = 1\n",
        "    data[i,4,b] = 1\n",
        "    data[i,5,b] = 1\n",
        "    data[i,6,b] = 1\n",
        "    data[i,7,b] = 1\n",
        "    data[i,8,b] = 1\n",
        "    data[i,9,b] = 1\n",
        "    data[i,10,b] = 1\n",
        "    data[i,11,b] = 1\n",
        "    data[i,12,b] = 1\n",
        "    data[i,13,b] = 1\n",
        "    data[i,14,b] = 1\n",
        "    data[i,15,b] = 1\n",
        "    data[i,16,b] = 1\n",
        "    data[i,17,b] = 1\n",
        "    data[i,18,b] = 1\n",
        "    data[i,19,b] = 1\n",
        "    data[i,20,b] = 1\n",
        "    data[i,21,b] = 1\n",
        "    data[i,22,b] = 1\n",
        "    data[i,23,b] = 1\n",
        "    #Week-in-a-year indices - Boolean - from feature #64 to #116 - 53 weeks\n",
        "    c = int(df.values[i,-1])+62\n",
        "    data[i,0,c] = 1\n",
        "    data[i,1,c] = 1\n",
        "    data[i,2,c] = 1\n",
        "    data[i,3,c] = 1\n",
        "    data[i,4,c] = 1\n",
        "    data[i,5,c] = 1\n",
        "    data[i,6,c] = 1\n",
        "    data[i,7,c] = 1\n",
        "    data[i,8,c] = 1\n",
        "    data[i,9,c] = 1\n",
        "    data[i,10,c] = 1\n",
        "    data[i,11,c] = 1\n",
        "    data[i,12,c] = 1\n",
        "    data[i,13,c] = 1\n",
        "    data[i,14,c] = 1\n",
        "    data[i,15,c] = 1\n",
        "    data[i,16,c] = 1\n",
        "    data[i,17,c] = 1\n",
        "    data[i,18,c] = 1\n",
        "    data[i,19,c] = 1\n",
        "    data[i,20,c] = 1\n",
        "    data[i,21,c] = 1\n",
        "    data[i,22,c] = 1\n",
        "    data[i,23,c] = 1\n",
        "    #Month-in-a-year indices - Boolean - from feature #117 to #128 - 12 months\n",
        "    d = int(df.values[i,1][-8:-6])+115\n",
        "    data[i,0,d] = 1\n",
        "    data[i,1,d] = 1\n",
        "    data[i,2,d] = 1\n",
        "    data[i,3,d] = 1\n",
        "    data[i,4,d] = 1\n",
        "    data[i,5,d] = 1\n",
        "    data[i,6,d] = 1\n",
        "    data[i,7,d] = 1\n",
        "    data[i,8,d] = 1\n",
        "    data[i,9,d] = 1\n",
        "    data[i,10,d] = 1\n",
        "    data[i,11,d] = 1\n",
        "    data[i,12,d] = 1\n",
        "    data[i,13,d] = 1\n",
        "    data[i,14,d] = 1\n",
        "    data[i,15,d] = 1\n",
        "    data[i,16,d] = 1\n",
        "    data[i,17,d] = 1\n",
        "    data[i,18,d] = 1\n",
        "    data[i,19,d] = 1\n",
        "    data[i,20,d] = 1\n",
        "    data[i,21,d] = 1\n",
        "    data[i,22,d] = 1\n",
        "    data[i,23,d] = 1\n",
        "    #Quarter-in-a-year indices - Boolean - from feature #128 to #131 - 4 quarters\n",
        "    if int(df.values[i,1][-8:-6]) in [1,2,3]:\n",
        "        data[i,0,128] = 1\n",
        "        data[i,1,128] = 1\n",
        "        data[i,2,128] = 1\n",
        "        data[i,3,128] = 1\n",
        "        data[i,4,128] = 1\n",
        "        data[i,5,128] = 1\n",
        "        data[i,6,128] = 1\n",
        "        data[i,7,128] = 1\n",
        "        data[i,8,128] = 1\n",
        "        data[i,9,128] = 1\n",
        "        data[i,10,128] = 1\n",
        "        data[i,11,128] = 1\n",
        "        data[i,12,128] = 1\n",
        "        data[i,13,128] = 1\n",
        "        data[i,14,128] = 1\n",
        "        data[i,15,128] = 1\n",
        "        data[i,16,128] = 1\n",
        "        data[i,17,128] = 1\n",
        "        data[i,18,128] = 1\n",
        "        data[i,19,128] = 1\n",
        "        data[i,20,128] = 1\n",
        "        data[i,21,128] = 1\n",
        "        data[i,22,128] = 1\n",
        "        data[i,23,128] = 1\n",
        "    elif int(df.values[i,1][-8:-6]) in [4,5,6]:\n",
        "        data[i,0,129] = 1\n",
        "        data[i,1,129] = 1\n",
        "        data[i,2,129] = 1\n",
        "        data[i,3,129] = 1\n",
        "        data[i,4,129] = 1\n",
        "        data[i,5,129] = 1\n",
        "        data[i,6,129] = 1\n",
        "        data[i,7,129] = 1\n",
        "        data[i,8,129] = 1\n",
        "        data[i,9,129] = 1\n",
        "        data[i,10,129] = 1\n",
        "        data[i,11,129] = 1\n",
        "        data[i,12,129] = 1\n",
        "        data[i,13,129] = 1\n",
        "        data[i,14,129] = 1\n",
        "        data[i,15,129] = 1\n",
        "        data[i,16,129] = 1\n",
        "        data[i,17,129] = 1\n",
        "        data[i,18,129] = 1\n",
        "        data[i,19,129] = 1\n",
        "        data[i,20,129] = 1\n",
        "        data[i,21,129] = 1\n",
        "        data[i,22,129] = 1\n",
        "        data[i,23,129] = 1\n",
        "    elif int(df.values[i,1][-8:-6]) in [7,8,9]:\n",
        "        data[i,0,130] = 1\n",
        "        data[i,1,130] = 1\n",
        "        data[i,2,130] = 1\n",
        "        data[i,3,130] = 1\n",
        "        data[i,4,130] = 1\n",
        "        data[i,5,130] = 1\n",
        "        data[i,6,130] = 1\n",
        "        data[i,7,130] = 1\n",
        "        data[i,8,130] = 1\n",
        "        data[i,9,130] = 1\n",
        "        data[i,10,130] = 1\n",
        "        data[i,11,130] = 1\n",
        "        data[i,12,130] = 1\n",
        "        data[i,13,130] = 1\n",
        "        data[i,14,130] = 1\n",
        "        data[i,15,130] = 1\n",
        "        data[i,16,130] = 1\n",
        "        data[i,17,130] = 1\n",
        "        data[i,18,130] = 1\n",
        "        data[i,19,130] = 1\n",
        "        data[i,20,130] = 1\n",
        "        data[i,21,130] = 1\n",
        "        data[i,22,130] = 1\n",
        "        data[i,23,130] = 1\n",
        "    else:\n",
        "        data[i,0,131] = 1\n",
        "        data[i,1,131] = 1\n",
        "        data[i,2,131] = 1\n",
        "        data[i,3,131] = 1\n",
        "        data[i,4,131] = 1\n",
        "        data[i,5,131] = 1\n",
        "        data[i,6,131] = 1\n",
        "        data[i,7,131] = 1\n",
        "        data[i,8,131] = 1\n",
        "        data[i,9,131] = 1\n",
        "        data[i,10,131] = 1\n",
        "        data[i,11,131] = 1\n",
        "        data[i,12,131] = 1\n",
        "        data[i,13,131] = 1\n",
        "        data[i,14,131] = 1\n",
        "        data[i,15,131] = 1\n",
        "        data[i,16,131] = 1\n",
        "        data[i,17,131] = 1\n",
        "        data[i,18,131] = 1\n",
        "        data[i,19,131] = 1\n",
        "        data[i,20,131] = 1\n",
        "        data[i,21,131] = 1\n",
        "        data[i,22,131] = 1\n",
        "        data[i,23,131] = 1\n",
        "        \n",
        "#Output a zip file that worked with Python .npz\n",
        "np.save('DataDec23_new.npz',data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRqKtkeDRcro",
        "colab_type": "text"
      },
      "source": [
        "## Data analysis\n",
        "This section cannot provide everything has been done in the Load data analysis part in our paper, but can only give the functions that are used to provide the insights that has been introduced.\n",
        "This section guides you through array differencing, PDF calculation function, PCA and Dendrogram calculating and plotting functions with assumptions that we want to plot the PDF of the whole dataset after wrangling and cluster its distribution into separate components as well as group further the same behavior sub-partition of each component with dendrogram.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwgCpHSKSIAE",
        "colab_type": "text"
      },
      "source": [
        "### Differencing function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlJvMysbRsX1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load data\n",
        "a = np.load('DataDec23_new.npz')\n",
        "data_all = a['arr_0']\n",
        "\n",
        "# Differencing with the scaling function log\n",
        "df_data_all = pd.Series(data_all[:,:,0].flatten())\n",
        "ts_log = np.log(df_data_all)\n",
        "diff = ts_log - ts_log.shift(24)\n",
        "diff_drop = np.array(diff[27768:]) # the array data is cut off at 27768, \n",
        "#which is the position of the time point January 1st, 2014"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhnsB5iPWqNw",
        "colab_type": "text"
      },
      "source": [
        "### PDF calculating and plotting functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nv58cnziWtjB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Plot the PDF function\n",
        "plt.hist(diff_drop,bins='sqrt',histtype = 'step',color='r')\n",
        "plt.title(\"PDF of the Difference Series\")\n",
        "plt.xlabel('Amplitude')\n",
        "plt.ylabel('Density')\n",
        "plt.show()\n",
        "plt.plot(diff_drop)\n",
        "plt.show()\n",
        "plt.boxplot(diff_drop)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6P0U4wEZayQ",
        "colab_type": "text"
      },
      "source": [
        "### PCA function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1QVg2CBZc_j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=2) #The number of components defines the dimension of the decomposition matrix, here we have a PCA 2D matrix to indicate the eigen position of each data points \n",
        "principalComponents = pca.fit_transform(diff_main[:,:,0].reshape(-1,24))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a330dbmcaU3n",
        "colab_type": "text"
      },
      "source": [
        "### Dendrogram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TV5Bg_2aYD4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.cluster import hierarchy\n",
        "hierarchy.dendrogram(diff_main[:,:,0].reshape(-1,24).T) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qscUZCJ4dyD4",
        "colab_type": "text"
      },
      "source": [
        "## The main code for proposed statistical data filtering method with confidence level\n",
        "- To integrate the algorithm to make the code runnable, confidence level must be converted to Sigma values.\n",
        "- Conversion from confidence levels to Sigma values:\n",
        "  - 90%: 1.65 sigma\n",
        "  - 91%: 1.69 sigma\n",
        "  - 92%: 1.75\n",
        "  - 93%: 1.81\n",
        "  - 94%: 1.89\n",
        "  - 95%: 1.96 sigma\n",
        "  - 96%: 2.05\n",
        "  - 97%: 2.17\n",
        "  - 98%: 2.33\n",
        "  - 99%: 2.575 sigma\n",
        "  - 99.73%: 3 sigma\n",
        "  - 99.9936%: 4 sigma\n",
        "  - 99.99932%: 4.5 sigma"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12H0S2knfGQ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# calculate summary statistics\n",
        "data_mean, data_std = mean(data_all[:,:,0].reshape(-1)), std(data_all[:,:,0].reshape(-1))\n",
        "# identify outliers\n",
        "cut_off = data_std * 1.65\n",
        "lower, upper = data_mean - cut_off, data_mean + cut_off\n",
        "\n",
        "for i in range(data_all.shape[0]):\n",
        "    for j in range(data_all.shape[1]):\n",
        "        if data_all[i,j,0] < lower:\n",
        "            data_all[i,j,0]= lower\n",
        "        elif data_all[i,j,0] > upper:\n",
        "            data_all[i,j,0] = upper\n",
        "print(\"1\")\n",
        "plt.hist(data_all[:,:,0].reshape(-1),bins='sqrt',histtype = 'step',color='r')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSJqAwun8saW",
        "colab_type": "text"
      },
      "source": [
        "## Test the filtered data with ANN model and print its MAPE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfqyH0LJ9iOS",
        "colab_type": "text"
      },
      "source": [
        "### Sunday"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6tzDitd81Ik",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from numpy.random import seed\n",
        "seed(1611)\n",
        "from tensorflow import set_random_seed\n",
        "set_random_seed(3116)\n",
        "\n",
        "# %% Load data \n",
        "\n",
        "a1 = np.load('SUN_to4_95.npz')\n",
        "data_all1 = a1['arr_0']\n",
        "te1 = np.load('SUN_to4.npz')\n",
        "data_te1 = te1['arr_0']\n",
        "a2 = np.load('SUN_5to6_95.npz')\n",
        "data_all2 = a2['arr_0']\n",
        "te2 = np.load('SUN_5to6.npz')\n",
        "data_te2 = te2['arr_0']\n",
        "a3 = np.load('SUN_7to15_95.npz')\n",
        "data_all3 = a3['arr_0']\n",
        "te3 = np.load('SUN_7to15.npz')\n",
        "data_te3 = te3['arr_0']\n",
        "a4 = np.load('SUN_16to23_95.npz')\n",
        "data_all4 = a4['arr_0']\n",
        "te4 = np.load('SUN_16to23.npz')\n",
        "data_te4 = te4['arr_0']\n",
        "\n",
        "data_all = np.zeros((data_all1.shape[0],24,133))\n",
        "data_all[:,:5,:] = data_all1\n",
        "data_all[:,5:7,:] = data_all2\n",
        "data_all[:,7:16,:] = data_all3\n",
        "data_all[:,16:,:] = data_all4\n",
        "data_te = np.zeros((data_te1.shape[0],24,133))\n",
        "data_te[:,:5,:] = data_te1\n",
        "data_te[:,5:7,:] = data_te2\n",
        "data_te[:,7:16,:] = data_te3\n",
        "data_te[:,16:,:] = data_te4\n",
        "\n",
        "z = np.load('presun_orig.npz')\n",
        "orig_test_sun = z['arr_0']\n",
        "z = np.load('sun_orig.npz')\n",
        "orig_test_sun_out = z['arr_0']\n",
        "\n",
        "# %% Split train, test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "inp = data_all[:-int(0.1*len(data_all))-1]\n",
        "out = data_all[1:-int(0.1*len(data_all))]\n",
        "test_inp = data_te[-int(0.1*len(data_all)):-1]\n",
        "test_out = data_te[-int(0.1*len(data_all))+1:]\n",
        "pretest_inp = orig_test_sun[-int(0.1*len(data_all))+1:]\n",
        "pretest_out = orig_test_sun_out[-int(0.1*len(data_all))+1:]\n",
        "x_train, x_val, y_train, y_val = train_test_split(inp,out,\n",
        "                                                  random_state=42, test_size =0.33,shuffle=False)\n",
        "\n",
        "\n",
        "\n",
        "# design network\n",
        "model = Sequential()\n",
        "model.add(Dense(100, input_shape=(data_all.shape[1],133)))\n",
        "model.add(Activation('selu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(100)) \n",
        "model.add(Activation('selu'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(data_all.shape[1]))\n",
        "rms = RMSprop()\n",
        "# Compile\n",
        "model.compile(loss='mae', optimizer='adam', metrics=['mse','mape'])\n",
        "# fit network\n",
        "history = model.fit(x_train, y_train[:,:,0], epochs=142, batch_size=4, validation_data=(x_val, y_val[:,:,0]), verbose=1, shuffle=True)\n",
        "\n",
        "# calculate MAPE\n",
        "AVG = []\n",
        "pred = []\n",
        "plot_val = []\n",
        "for i in range(len(test_inp)):\n",
        "    test_pred = model.predict(test_inp[i].reshape(-1,data_all.shape[1],133))\n",
        "    pred.append(test_pred)\n",
        "    real_pred = np.log(pretest_inp[i,:,0].T) + test_pred\n",
        "    real_pred = np.e**real_pred\n",
        "    print(abs((real_pred.T.reshape(-1)-pretest_out[i,:,0].reshape(-1)))/pretest_out[i,:,0].reshape(-1))\n",
        "    plot_val.append([real_pred.T.reshape(-1),pretest_out[i,:,0].reshape(-1)])\n",
        "    AVG.append(np.sum(abs((real_pred.T.reshape(-1)-pretest_out[i,:,0].reshape(-1)))/pretest_out[i,:,0].reshape(-1))*100/24)\n",
        "AVG = np.array(AVG)\n",
        "pred=np.array(pred)\n",
        "print(np.sum(AVG)/len(pretest_inp))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YXZf-r79zJ1",
        "colab_type": "text"
      },
      "source": [
        "### Monday"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLXGdsW090Or",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from numpy.random import seed\n",
        "seed(1611)\n",
        "from tensorflow import set_random_seed\n",
        "set_random_seed(3116)\n",
        "\n",
        "# %% Load data \n",
        "\n",
        "a1 = np.load('MON_to4_95.npz')\n",
        "data_all1 = a1['arr_0']\n",
        "te1 = np.load('MON_to4.npz')\n",
        "data_te1 = te1['arr_0']\n",
        "a2 = np.load('MON_5to6_95.npz')\n",
        "data_all2 = a2['arr_0']\n",
        "te2 = np.load('MON_5to6.npz')\n",
        "data_te2 = te2['arr_0']\n",
        "a3 = np.load('MON_7to16_95.npz')\n",
        "data_all3 = a3['arr_0']\n",
        "te3 = np.load('MON_7to16.npz')\n",
        "data_te3 = te3['arr_0']\n",
        "a4 = np.load('MON_17to23_95.npz')\n",
        "data_all4 = a4['arr_0']\n",
        "te4 = np.load('MON_17to23.npz')\n",
        "data_te4 = te4['arr_0']\n",
        "\n",
        "data_all = np.zeros((data_all1.shape[0],24,133))\n",
        "data_all[:,:5,:] = data_all1\n",
        "data_all[:,5:7,:] = data_all2\n",
        "data_all[:,7:17,:] = data_all3\n",
        "data_all[:,17:,:] = data_all4\n",
        "data_te = np.zeros((data_te1.shape[0],24,133))\n",
        "data_te[:,:5,:] = data_te1\n",
        "data_te[:,5:7,:] = data_te2\n",
        "data_te[:,7:17,:] = data_te3\n",
        "data_te[:,17:,:] = data_te4\n",
        "\n",
        "z = np.load('premon_orig.npz')\n",
        "orig_test_sun = z['arr_0']\n",
        "z = np.load('mon_orig.npz')\n",
        "orig_test_sun_out = z['arr_0']\n",
        "\n",
        "# %% Split train, test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "inp = data_all[:-int(0.1*len(data_all))-1]\n",
        "out = data_all[1:-int(0.1*len(data_all))]\n",
        "test_inp = data_te[-int(0.1*len(data_all)):-1]\n",
        "test_out = data_te[-int(0.1*len(data_all))+1:]\n",
        "pretest_inp = orig_test_mon[-int(0.1*len(data_all))+1:]\n",
        "pretest_out = orig_test_mon_out[-int(0.1*len(data_all))+1:]\n",
        "x_train, x_val, y_train, y_val = train_test_split(inp,out,\n",
        "                                                  random_state=42, test_size =0.33,shuffle=False)\n",
        "\n",
        "\n",
        "\n",
        "# design network\n",
        "model = Sequential()\n",
        "model.add(Dense(100, input_shape=(data_all.shape[1],133)))\n",
        "model.add(Activation('selu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(100)) \n",
        "model.add(Activation('selu'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(data_all.shape[1]))\n",
        "rms = RMSprop()\n",
        "# Compile\n",
        "model.compile(loss='mae', optimizer='adam', metrics=['mse','mape'])\n",
        "# fit network\n",
        "history = model.fit(x_train, y_train[:,:,0], epochs=142, batch_size=4, validation_data=(x_val, y_val[:,:,0]), verbose=1, shuffle=True)\n",
        "\n",
        "# calculate MAPE\n",
        "AVG = []\n",
        "pred = []\n",
        "plot_val = []\n",
        "for i in range(len(test_inp)):\n",
        "    test_pred = model.predict(test_inp[i].reshape(-1,data_all.shape[1],133))\n",
        "    pred.append(test_pred)\n",
        "    real_pred = np.log(pretest_inp[i,:,0].T) + test_pred\n",
        "    real_pred = np.e**real_pred\n",
        "    print(abs((real_pred.T.reshape(-1)-pretest_out[i,:,0].reshape(-1)))/pretest_out[i,:,0].reshape(-1))\n",
        "    plot_val.append([real_pred.T.reshape(-1),pretest_out[i,:,0].reshape(-1)])\n",
        "    AVG.append(np.sum(abs((real_pred.T.reshape(-1)-pretest_out[i,:,0].reshape(-1)))/pretest_out[i,:,0].reshape(-1))*100/24)\n",
        "AVG = np.array(AVG)\n",
        "pred=np.array(pred)\n",
        "print(np.sum(AVG)/len(pretest_inp))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTFBuaFJ-DrN",
        "colab_type": "text"
      },
      "source": [
        "### The rests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_p90wju-GHZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from numpy.random import seed\n",
        "seed(1611)\n",
        "from tensorflow import set_random_seed\n",
        "set_random_seed(3116)\n",
        "\n",
        "# %% Load data\n",
        "\n",
        "\n",
        "a1 = np.load('RESTs_95.npz')\n",
        "data_all = a1['arr_0']\n",
        "te1 = np.load('RESTs.npz')\n",
        "data_te = te1['arr_0']\n",
        "\n",
        "r = np.load('rests_orig.npz')\n",
        "orig_rests = r['arr_0']\n",
        "orig_test_rests = orig_rests[:-1]\n",
        "orig_test_rests_out = orig_rests[1:]\n",
        "\n",
        "# %% Split train, test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "inp = data_all[:-int(0.1*len(data_all))-1]\n",
        "out = data_all[1:-int(0.1*len(data_all))]\n",
        "test_inp = data_te[-int(0.1*len(data_all)):-1]\n",
        "test_out = data_te[-int(0.1*len(data_all))+1:]\n",
        "pretest_inp = orig_test_rests[-int(0.1*len(data_all))+1:]\n",
        "pretest_out = orig_test_rests_out[-int(0.1*len(data_all))+1:]\n",
        "x_train, x_val, y_train, y_val = train_test_split(inp,out,\n",
        "                                                  random_state=42, test_size =0.33,shuffle=False)\n",
        "\n",
        "\n",
        "\n",
        "# design network\n",
        "model = Sequential()\n",
        "model.add(Dense(100, input_shape=(data_all.shape[1],133)))\n",
        "model.add(Activation('selu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(100)) \n",
        "model.add(Activation('selu'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(data_all.shape[1]))\n",
        "rms = RMSprop()\n",
        "# Compile\n",
        "model.compile(loss='mae', optimizer='adam', metrics=['mse','mape'])\n",
        "# fit network\n",
        "history = model.fit(x_train, y_train[:,:,0], epochs=142, batch_size=4, validation_data=(x_val, y_val[:,:,0]), verbose=1, shuffle=True)\n",
        "\n",
        "# calculate MAPE\n",
        "AVG = []\n",
        "pred = []\n",
        "plot_val = []\n",
        "for i in range(len(test_inp)):\n",
        "    test_pred = model.predict(test_inp[i].reshape(-1,data_all.shape[1],133))\n",
        "    pred.append(test_pred)\n",
        "    real_pred = np.log(pretest_inp[i,:,0].T) + test_pred\n",
        "    real_pred = np.e**real_pred\n",
        "    print(abs((real_pred.T.reshape(-1)-pretest_out[i,:,0].reshape(-1)))/pretest_out[i,:,0].reshape(-1))\n",
        "    plot_val.append([real_pred.T.reshape(-1),pretest_out[i,:,0].reshape(-1)])\n",
        "    AVG.append(np.sum(abs((real_pred.T.reshape(-1)-pretest_out[i,:,0].reshape(-1)))/pretest_out[i,:,0].reshape(-1))*100/24)\n",
        "AVG = np.array(AVG)\n",
        "pred=np.array(pred)\n",
        "print(np.sum(AVG)/len(pretest_inp))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjZVxMYX-Z2r",
        "colab_type": "text"
      },
      "source": [
        "## Test the filtered data with ARIMA model and print its MAPE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzcaBaPn-lrE",
        "colab_type": "text"
      },
      "source": [
        "### Sunday"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bud9tb-i-bBb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from numpy.random import seed\n",
        "seed(1611)\n",
        "from tensorflow import set_random_seed\n",
        "set_random_seed(3116)\n",
        "\n",
        "# %% Load data \n",
        "\n",
        "a1 = np.load('SUN_to4_95.npz')\n",
        "data_all1 = a1['arr_0']\n",
        "te1 = np.load('SUN_to4.npz')\n",
        "data_te1 = te1['arr_0']\n",
        "a2 = np.load('SUN_5to6_95.npz')\n",
        "data_all2 = a2['arr_0']\n",
        "te2 = np.load('SUN_5to6.npz')\n",
        "data_te2 = te2['arr_0']\n",
        "a3 = np.load('SUN_7to15_95.npz')\n",
        "data_all3 = a3['arr_0']\n",
        "te3 = np.load('SUN_7to15.npz')\n",
        "data_te3 = te3['arr_0']\n",
        "a4 = np.load('SUN_16to23_95.npz')\n",
        "data_all4 = a4['arr_0']\n",
        "te4 = np.load('SUN_16to23.npz')\n",
        "data_te4 = te4['arr_0']\n",
        "\n",
        "data_all = np.zeros((data_all1.shape[0],24,133))\n",
        "data_all[:,:5,:] = data_all1\n",
        "data_all[:,5:7,:] = data_all2\n",
        "data_all[:,7:16,:] = data_all3\n",
        "data_all[:,16:,:] = data_all4\n",
        "data_te = np.zeros((data_te1.shape[0],24,133))\n",
        "data_te[:,:5,:] = data_te1\n",
        "data_te[:,5:7,:] = data_te2\n",
        "data_te[:,7:16,:] = data_te3\n",
        "data_te[:,16:,:] = data_te4\n",
        "\n",
        "z = np.load('presun_orig.npz')\n",
        "orig_test_sun = z['arr_0']\n",
        "z = np.load('sun_orig.npz')\n",
        "orig_test_sun_out = z['arr_0']\n",
        "\n",
        "# %% Split train, test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "inp = data_all[:-int(0.1*len(data_all))-1]\n",
        "out = data_all[1:-int(0.1*len(data_all))]\n",
        "test_inp = data_te[-int(0.1*len(data_all)):-1]\n",
        "test_out = data_te[-int(0.1*len(data_all))+1:]\n",
        "pretest_inp = orig_test_sun[-int(0.1*len(data_all))+1:]\n",
        "pretest_out = orig_test_sun_out[-int(0.1*len(data_all))+1:]\n",
        "x_train, x_val, y_train, y_val = train_test_split(inp,out,\n",
        "                                                  random_state=42, test_size =0.33,shuffle=False)\n",
        "\n",
        "\n",
        "\n",
        "from statsmodels.tsa.arima_model import ARIMA\n",
        "AVG=[]\n",
        "history = [x for x in data_all[:,:,0].reshape(-1)]\n",
        "predictions = list()\n",
        "plot_val1=[]\n",
        "for t in range(len(pretest_out[:,:,0].reshape(-1))):\n",
        "  model = ARIMA(history, order=(6,0,0))\n",
        "  model_fit = model.fit(disp=1)\n",
        "  output = model_fit.forecast()\n",
        "  \n",
        "  \n",
        "  yhat = output[0]\n",
        "  test_pred = yhat\n",
        "  real_pred = np.log(pretest_inp[:,:,0].reshape(-1)[t])+test_pred\n",
        "  real_pred = np.e**real_pred\n",
        "  print(test_pred.shape)\n",
        "  #pred.append(test_pred)\n",
        "  print(abs((real_pred-pretest_out[:,:,0].reshape(-1)[t]))/pretest_out[:,:,0].reshape(-1)[t])\n",
        "  plot_val1.append([real_pred,pretest_out[:,:,0].reshape(-1)[t]])\n",
        "  AVG.append(abs((real_pred-pretest_out[:,:,0].reshape(-1)[t]))/pretest_out[:,:,0].reshape(-1)[t]*100)\n",
        "AVG = np.array(AVG)\n",
        "#pred=np.array(pred)\n",
        "print(np.mean(AVG))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4-9i_yb-0F_",
        "colab_type": "text"
      },
      "source": [
        "### Monday"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VTAg8W3-1Mc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from numpy.random import seed\n",
        "seed(1611)\n",
        "from tensorflow import set_random_seed\n",
        "set_random_seed(3116)\n",
        "\n",
        "# %% Load data \n",
        "\n",
        "a1 = np.load('MON_to4_95.npz')\n",
        "data_all1 = a1['arr_0']\n",
        "te1 = np.load('MON_to4.npz')\n",
        "data_te1 = te1['arr_0']\n",
        "a2 = np.load('MON_5to6_95.npz')\n",
        "data_all2 = a2['arr_0']\n",
        "te2 = np.load('MON_5to6.npz')\n",
        "data_te2 = te2['arr_0']\n",
        "a3 = np.load('MON_7to16_95.npz')\n",
        "data_all3 = a3['arr_0']\n",
        "te3 = np.load('MON_7to16.npz')\n",
        "data_te3 = te3['arr_0']\n",
        "a4 = np.load('MON_17to23_95.npz')\n",
        "data_all4 = a4['arr_0']\n",
        "te4 = np.load('MON_17to23.npz')\n",
        "data_te4 = te4['arr_0']\n",
        "\n",
        "data_all = np.zeros((data_all1.shape[0],24,133))\n",
        "data_all[:,:5,:] = data_all1\n",
        "data_all[:,5:7,:] = data_all2\n",
        "data_all[:,7:17,:] = data_all3\n",
        "data_all[:,17:,:] = data_all4\n",
        "data_te = np.zeros((data_te1.shape[0],24,133))\n",
        "data_te[:,:5,:] = data_te1\n",
        "data_te[:,5:7,:] = data_te2\n",
        "data_te[:,7:17,:] = data_te3\n",
        "data_te[:,17:,:] = data_te4\n",
        "\n",
        "z = np.load('premon_orig.npz')\n",
        "orig_test_sun = z['arr_0']\n",
        "z = np.load('mon_orig.npz')\n",
        "orig_test_sun_out = z['arr_0']\n",
        "\n",
        "# %% Split train, test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "inp = data_all[:-int(0.1*len(data_all))-1]\n",
        "out = data_all[1:-int(0.1*len(data_all))]\n",
        "test_inp = data_te[-int(0.1*len(data_all)):-1]\n",
        "test_out = data_te[-int(0.1*len(data_all))+1:]\n",
        "pretest_inp = orig_test_mon[-int(0.1*len(data_all))+1:]\n",
        "pretest_out = orig_test_mon_out[-int(0.1*len(data_all))+1:]\n",
        "x_train, x_val, y_train, y_val = train_test_split(inp,out,\n",
        "                                                  random_state=42, test_size =0.33,shuffle=False)\n",
        "\n",
        "\n",
        "\n",
        "from statsmodels.tsa.arima_model import ARIMA\n",
        "AVG=[]\n",
        "history = [x for x in data_all[:,:,0].reshape(-1)]\n",
        "predictions = list()\n",
        "plot_val1=[]\n",
        "for t in range(len(pretest_out[:,:,0].reshape(-1))):\n",
        "  model = ARIMA(history, order=(6,0,0))\n",
        "  model_fit = model.fit(disp=1)\n",
        "  output = model_fit.forecast()\n",
        "  \n",
        "  \n",
        "  yhat = output[0]\n",
        "  test_pred = yhat\n",
        "  real_pred = np.log(pretest_inp[:,:,0].reshape(-1)[t])+test_pred\n",
        "  real_pred = np.e**real_pred\n",
        "  print(test_pred.shape)\n",
        "  #pred.append(test_pred)\n",
        "  print(abs((real_pred-pretest_out[:,:,0].reshape(-1)[t]))/pretest_out[:,:,0].reshape(-1)[t])\n",
        "  plot_val1.append([real_pred,pretest_out[:,:,0].reshape(-1)[t]])\n",
        "  AVG.append(abs((real_pred-pretest_out[:,:,0].reshape(-1)[t]))/pretest_out[:,:,0].reshape(-1)[t]*100)\n",
        "AVG = np.array(AVG)\n",
        "#pred=np.array(pred)\n",
        "print(np.mean(AVG))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeVLUPEb_DiV",
        "colab_type": "text"
      },
      "source": [
        "### The rests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VF4yYMAS_Eol",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from numpy.random import seed\n",
        "seed(1611)\n",
        "from tensorflow import set_random_seed\n",
        "set_random_seed(3116)\n",
        "\n",
        "# %% Load data\n",
        "\n",
        "\n",
        "a1 = np.load('RESTs_95.npz')\n",
        "data_all = a1['arr_0']\n",
        "te1 = np.load('RESTs.npz')\n",
        "data_te = te1['arr_0']\n",
        "\n",
        "r = np.load('rests_orig.npz')\n",
        "orig_rests = r['arr_0']\n",
        "orig_test_rests = orig_rests[:-1]\n",
        "orig_test_rests_out = orig_rests[1:]\n",
        "\n",
        "# %% Split train, test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "inp = data_all[:-int(0.1*len(data_all))-1]\n",
        "out = data_all[1:-int(0.1*len(data_all))]\n",
        "test_inp = data_te[-int(0.1*len(data_all)):-1]\n",
        "test_out = data_te[-int(0.1*len(data_all))+1:]\n",
        "pretest_inp = orig_test_rests[-int(0.1*len(data_all))+1:]\n",
        "pretest_out = orig_test_rests_out[-int(0.1*len(data_all))+1:]\n",
        "x_train, x_val, y_train, y_val = train_test_split(inp,out,\n",
        "                                                  random_state=42, test_size =0.33,shuffle=False)\n",
        "\n",
        "\n",
        "\n",
        "from statsmodels.tsa.arima_model import ARIMA\n",
        "AVG=[]\n",
        "history = [x for x in data_all[:,:,0].reshape(-1)]\n",
        "predictions = list()\n",
        "plot_val1=[]\n",
        "for t in range(len(pretest_out[:,:,0].reshape(-1))):\n",
        "  model = ARIMA(history, order=(6,0,0))\n",
        "  model_fit = model.fit(disp=1)\n",
        "  output = model_fit.forecast()\n",
        "  \n",
        "  \n",
        "  yhat = output[0]\n",
        "  test_pred = yhat\n",
        "  real_pred = np.log(pretest_inp[:,:,0].reshape(-1)[t])+test_pred\n",
        "  real_pred = np.e**real_pred\n",
        "  print(test_pred.shape)\n",
        "  #pred.append(test_pred)\n",
        "  print(abs((real_pred-pretest_out[:,:,0].reshape(-1)[t]))/pretest_out[:,:,0].reshape(-1)[t])\n",
        "  plot_val1.append([real_pred,pretest_out[:,:,0].reshape(-1)[t]])\n",
        "  AVG.append(abs((real_pred-pretest_out[:,:,0].reshape(-1)[t]))/pretest_out[:,:,0].reshape(-1)[t]*100)\n",
        "AVG = np.array(AVG)\n",
        "#pred=np.array(pred)\n",
        "print(np.mean(AVG))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}